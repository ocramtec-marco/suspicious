{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa6e5e1",
   "metadata": {},
   "source": [
    "# Train models for Suspicious\n",
    "\n",
    "Use this python notebook to train your models for the AIMailAnalyzer, from a csv dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff72c8f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c29062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dff003",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba108b",
   "metadata": {},
   "source": [
    "Set the parameters for the training and data processing. You can change the parameters dependings on your needs and the data you have.\n",
    "\n",
    "#### Data\n",
    "\n",
    "- **CSV_DATASET**: List of CSV files to load as datasets. Each file should contain a column named 'body' with the email content and a column named 'label' with the label for each email.\n",
    "- **y_encoder**: A dictionary to encode the labels to integers. The labels needs to match the labels in your dataset ('label' column of the csv). For example:\n",
    "  ```python\n",
    "  y_encoder = {\n",
    "    '0_LEGIT_INTERNAL_COMMUNICATION': 0,\n",
    "    '0_LEGIT_EXTERNAL_COMMUNICATION': 1,\n",
    "    '1_SPAM': 2,\n",
    "    '1_NEWSLETTER': 3,\n",
    "    '2_CLASSIC_PHISHING': 4,\n",
    "    '2_WHALING_PHISHING': 5,\n",
    "    '2_CLONE_PHISHING': 6,\n",
    "    '2_BLACKMAILING_PHISHING': 7,\n",
    "  }\n",
    "  ```\n",
    "- **LABELS**: List of labels to use for the classification. These should match the folder names in your Suspicious mailbox.\n",
    "- **VECTORIZER**: The model to use for vectorizing the emails.\n",
    "\n",
    "#### Data visualization\n",
    "\n",
    "- **class_labels**: List of class labels for visualization.\n",
    "- **class_colors**: List colors for each class label.\n",
    "- **safe_suspicious_labels**: List of labels for safe vs suspicious classification.\n",
    "- **safe_suspicious_colors**: List of colors for safe vs suspicious classification.\n",
    "- **unwanted_dangerous_labels**: List of labels for unwanted vs dangerous classification.\n",
    "- **unwanted_dangerous_colors**: List of colors for unwanted vs dangerous classification.\n",
    "- **safe_unwanted_dangerous_labels**: List of labels for safe vs unwanted vs dangerous classification.\n",
    "- **safe_unwanted_dangerous_colors**: List of colors for safe vs unwanted vs dangerous classification.\n",
    "- **safe_labels**: List of labels for safe classification.\n",
    "- **safe_colors**: List of colors for safe classification.\n",
    "- **unwanted_labels**: List of labels for unwanted classification.\n",
    "- **unwanted_colors**: List of colors for unwanted classification.\n",
    "- **dangerous_labels**: List of labels for dangerous classification.\n",
    "- **dangerous_colors**: List of colors for dangerous classification.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "class_labels = [\"internal\", \"external\", \"spam\", \"newsletter\", \"classic phishing\", \"whaling\", \"clone\", \"blackmail\"]\n",
    "class_colors = [\"mediumseagreen\", \"limegreen\", \"orange\", \"goldenrod\", \"red\", \"firebrick\", \"indianred\", \"lightcoral\"]\n",
    "safe_suspicious_labels = [\"safe\", \"suspicious\"]\n",
    "safe_suspicious_colors = [\"green\", \"orange\"]\n",
    "unwanted_dangerous_labels = [\"unwanted\", \"dangerous\"]\n",
    "unwanted_dangerous_colors = [\"gold\", \"red\"]\n",
    "safe_unwanted_dangerous_labels = [\"safe\", \"unwanted\", \"dangerous\"]\n",
    "safe_unwanted_dangerous_colors = [\"green\", \"gold\", \"red\"]\n",
    "safe_labels = [\"internal\", \"external\"]\n",
    "safe_colors = [\"mediumseagreen\", \"limegreen\"]\n",
    "unwanted_labels = [\"spam\", \"newsletter\"]\n",
    "unwanted_colors = [\"gold\", \"khaki\"]\n",
    "dangerous_labels = [\"classic phishing\", \"whaling\", \"clone\", \"blackmail\"]\n",
    "dangerous_colors = [\"red\", \"firebrick\", \"indianred\", \"lightcoral\"]\n",
    "```\n",
    "\n",
    "#### Training\n",
    "\n",
    "- **TEST_SIZE**: The proportion of the dataset to include in the test\n",
    "- **RANDOM_STATE**: Controls the shuffling applied to the data before applying the split. Pass an int for reproducible\n",
    "- **SMOTE_FACTOR**: The factor to increase the minority class by using SMOTE.\n",
    "- **ROS_FACTOR**: The factor to increase the minority class by using Random Over Sampling.\n",
    "- **CAP_MULT**: The factor to determine the maximum number of samples per class after capping.\n",
    "- **OVERSAMPLED_CSV**: If you oversampled emails an other way, you can use this csv file to load the oversampled dataset. `None` if you don't want to use it.\n",
    "- **BATCH_SIZE**: Number of samples per gradient update.\n",
    "- **LEARNING_RATE**: The learning rate for the model.\n",
    "- **EPOCHS**: Number of epochs to train the model.\n",
    "- **MODEL_OUTPUT_PATH**: Folder to save the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "CSV_DATASET = ''\n",
    "y_encoder = {\n",
    "    \n",
    "}\n",
    "LABELS = list(y_encoder.keys())\n",
    "VECTORIZER = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Data visualization\n",
    "class_labels = []\n",
    "class_colors = []\n",
    "safe_suspicious_labels = []\n",
    "safe_suspicious_colors = []\n",
    "unwanted_dangerous_labels = []\n",
    "unwanted_dangerous_colors = []\n",
    "safe_unwanted_dangerous_labels = []\n",
    "safe_unwanted_dangerous_colors = []\n",
    "safe_labels = []\n",
    "safe_colors = []\n",
    "unwanted_labels = []\n",
    "unwanted_colors = []\n",
    "dangerous_labels = []\n",
    "dangerous_colors = []\n",
    "\n",
    "# Training\n",
    "TEST_SIZE =  # Recommended: 0.2 - 0.3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "SMOTE_FACTOR =  # Recommended: 1.0 - 1.5\n",
    "ROS_FACTOR =  # Recommended: 1.0 - 1.25\n",
    "CAP_MULT =  # Recommended: 1.5 - 2.0\n",
    "OVERSAMPLED_CSV = None # Recommended: None or 'oversampled_data.csv'\n",
    "\n",
    "BATCH_SIZE =  # Recommended: 8 - 16\n",
    "LEARNING_RATE = # Recommended: 0.001 - 0.01\n",
    "EPOCHS =  # Recommended: 10 - 20\n",
    "\n",
    "# Note that the recommended values corresponds to a small imbalanced dataset (~1000 samples). For larger datasets, you can increase the batch size and decrease the number of epochs.\n",
    "\n",
    "MODEL_OUTPUT_PATH = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb64ce",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header_dict_list(msg):\n",
    "    headers = defaultdict(list)\n",
    "    for key, value in msg.items():\n",
    "        headers[key].append(value)\n",
    "    return headers\n",
    "\n",
    "def calculate_hash(data, hash_type=\"sha256\"):\n",
    "    \"\"\"Calculate the hash of given data.\"\"\"\n",
    "    if hash_type == \"sha256\":\n",
    "        hasher = hashlib.sha256()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported hash type: {hash_type}\")\n",
    "    \n",
    "    hasher.update(data)\n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7747de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_body(text: str) -> str:\n",
    "    patterns = [\n",
    "        (r\"\\u00A0|\\r\", \"\"),\n",
    "        (r\" +\\n\", \"\\n\"),\n",
    "        (r\"=\\n\", \"\"),\n",
    "        (r\"\\[cid:.*?\\]\\n?\", \"\"),\n",
    "        (\n",
    "            r\".*THALES (GROUP|ALENIA SPACE) (LIMITED DISTRIBUTION|CONFIDENTIAL).*\\n\",\n",
    "            \"\",\n",
    "        ),\n",
    "        (r\"Sensitivity:.*\\n\", \"\"),\n",
    "        (r\"Critère de diffusion ?:.*\\n\", \"\"),\n",
    "        (r\"-----------------------------------------------------------------------------------This email has been detected as potentially unwanted, make sure it is not a phishing attempt by visiting this page: Suspicious email reporting - Thales Cybersecurity (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>You will find there the procedure to follow if this message turns out to be a phishing attempt or spam. Otherwise, please ignore this message.”Ce mail a été détecté comme potentiellement indésirable, assurez-vous qu’il ne s’agit pas d’une tentative d’hameçonnage en  consultant cette page : Signalement des e-mails suspects - Thales Cybersecurité (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>Vous y trouverez la procédure à suivre si ce message s’avère être une tentative d’hameçonnage ou un spam. Si ce n’est pas le cas, merci d’ignorer ce message.-----------------------------------------------------------------------------------*\\n\", \"\"),\n",
    "        (r\"-----------------------------------------------------------------------------------This email has been detected as potentially unwanted, make sure it is not a phishing attempt by visiting this page: Suspicious email reporting - Thales Cybersecurity (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>You will find there the procedure to follow if this message turns out to be a phishing attempt or spam. Otherwise, please ignore this message.”Ce mail a été détecté comme potentiellement indésirable, assurez-vous qu’il ne s’agit pas d’une tentative d’hameçonnage en consultant cette page : Signalement des e-mails suspects - Thales Cybersecurité (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>Vous y trouverez la procédure à suivre si ce message s’avère être une tentative d’hameçonnage ou un spam. Si ce n’est pas le cas, merci d’ignorer ce message.-----------------------------------------------------------------------------------*\\n\", \"\"),\n",
    "        (r\"-----------------------------------------------------------------------------------This email has been detected as potentially unwanted, make sure it is not a phishing attempt by visiting this page: Suspicious email reporting - Thales Cybersecurity (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>You will find there the procedure to follow if this message turns out to be a phishing attempt or spam. Otherwise, please ignore this message.‚ÄùCe mail a √©t√© d√©tect√© comme potentiellement ind√©sirable, assurez-vous qu‚Äôil ne s‚Äôagit pas d‚Äôune tentative d‚Äôhame√ßonnage en consultant cette page : Signalement des e-mails suspects - Thales Cybersecurit√© (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>Vous y trouverez la proc√©dure √† suivre si ce message s‚Äôav√®re √™tre une tentative d‚Äôhame√ßonnage ou un spam. Si ce n‚Äôest pas le cas, merci d‚Äôignorer ce message.-----------------------------------------------------------------------------------*\\n\", \"\"),\n",
    "        (r\"-----------------------------------------------------------------------------------This email has been detected as potentially unwanted, make sure it is not a phishing attempt by visiting this page: Suspicious email reporting - Thales Cybersecurity (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>You will find there the procedure to follow if this message turns out to be a phishing attempt or spam. Otherwise, please ignore this message.\\\"Ce mail a été détecté comme potentiellement indésirable, assurez-vous qu'il ne s'agit pas d'une tentative d'hameçonnage en  consultant cette page : Signalement des e-mails suspects - Thales Cybersecurité (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>Vous y trouverez la procédure à suivre si ce message s'avère être une tentative d'hameçonnage ou un spam. Si ce n'est pas le cas, merci d'ignorer ce message.-----------------------------------------------------------------------------------*\\n\", \"\"),\n",
    "        (r\"-----------------------------------------------------------------------------------This email has been detected as potentially unwanted, make sure it is not a phishing attempt by visiting this page: Suspicious email reporting - Thales Cybersecurity (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>You will find there the procedure to follow if this message turns out to be a phishing attempt or spam. Otherwise, please ignore this message.\\\"Ce mail a été détecté comme potentiellement indésirable, assurez-vous qu'il ne s'agit pas d'une tentative d'hameçonnage en consultant cette page : Signalement des e-mails suspects - Thales Cybersecurité (corp.thales) <https://intranet.peopleonline.corp.thales/sites/group/transformation-and-development/thales-cybersecurity/suspicious-email-reporting>Vous y trouverez la procédure à suivre si ce message s'avère être une tentative d'hameçonnage ou un spam. Si ce n'est pas le cas, merci d'ignorer ce message.-----------------------------------------------------------------------------------*\\n\", \"\"),\n",
    "        (r\"\\n{3,}\", \"\\n\\n\"),\n",
    "        (r\"((From|De).*)\\n\\n\", r\"\\1\\n\"),\n",
    "        (r\"^\\s+|\\s+$\", \"\"),\n",
    "    ]\n",
    "    for pat, repl in patterns:\n",
    "        text = re.sub(pat, repl, text, flags=re.MULTILINE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_email_statistics_and_pca(X, y, label_names, colors, plot_title):\n",
    "    # Count the occurrences of each label (email type)\n",
    "    label_counts_dict = Counter(y)\n",
    "\n",
    "    # Create a list of unique labels and their counts\n",
    "    label_counts = [label_counts_dict.get(label_idx, 0) for label_idx in range(len(label_names))]\n",
    "\n",
    "    # Define a custom color map\n",
    "    color_map = {label_name: colors[i] for i, label_name in enumerate(label_names)}\n",
    "\n",
    "    # Map the colors to each label\n",
    "    label_color_list = [color_map[label_name] for label_name in label_names]\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[2, 3])\n",
    "\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "\n",
    "    # Bar chart for email counts\n",
    "    bars = ax0.bar(label_names, label_counts, color=label_color_list)\n",
    "    \n",
    "    # Add count labels on top of each bar\n",
    "    for bar, count in zip(bars, label_counts):\n",
    "        height = bar.get_height()\n",
    "        ax0.text(bar.get_x() + bar.get_width() / 2, height, str(count), ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "    ax0.set_ylabel('Count')\n",
    "    ax0.set_title('Count of Email Types')\n",
    "    ax0.set_ylim(0, 1000)  # Set y-axis limit from 0 to 1500\n",
    "\n",
    "    # Fit PCA on training data\n",
    "    pca = PCA(n_components=2)\n",
    "    X_flat = np.array([np.array(row).flatten() for row in X])\n",
    "    X_train_pca = pca.fit_transform(X_flat)\n",
    "\n",
    "    # PCA Visualization\n",
    "    scatter = ax1.scatter(\n",
    "        X_train_pca[:, 0],\n",
    "        X_train_pca[:, 1],\n",
    "        c=y,\n",
    "        cmap=LinearSegmentedColormap.from_list(\"custom_cmap\", colors),\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "    fig.colorbar(scatter, ax=ax1, label='Email Types')\n",
    "    ax1.set_title('PCA Projection of Resampled Training Data')\n",
    "    ax1.set_xlabel('Principal Component 1')\n",
    "    ax1.set_ylabel('Principal Component 2')\n",
    "\n",
    "    # Show the figure\n",
    "    plt.suptitle(plot_title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_similar_mails(df, field, threshold=0.9, debug=False):\n",
    "    vectors = np.vstack(df[field].values)\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    to_remove = set()\n",
    "    n = len(df)\n",
    "\n",
    "    for i in tqdm(range(n), desc=\"Removing similar mails\"):\n",
    "        if i in to_remove:\n",
    "            continue\n",
    "\n",
    "        similar_indices = []\n",
    "        for j in range(i + 1, n):\n",
    "            if j in to_remove:\n",
    "                continue\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            if similarity > threshold:\n",
    "                to_remove.add(j)\n",
    "                similar_indices.append((j, similarity))\n",
    "\n",
    "        if debug and similar_indices:\n",
    "            print(f\"\\n[DEBUG] Original: {df.iloc[i]['body'][:75]}...\")\n",
    "            for j, similarity in similar_indices:\n",
    "                print(f\"[DEBUG] Similar {similarity:.4f} (>{threshold}): {df.iloc[j]['body'][:75]}...\")\n",
    "\n",
    "    keep_indices = [i for i in range(n) if i not in to_remove]\n",
    "    df_cleaned = df.iloc[keep_indices].reset_index(drop=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_data(X_train, y_train, smote_factor=SMOTE_FACTOR, ros_factor=ROS_FACTOR, random_state=RANDOM_STATE, cap_mult=CAP_MULT):\n",
    "    original_counts = Counter(y_train)\n",
    "    mean_n = int(pd.Series(list(original_counts.values())).mean())\n",
    "    undersample_cap = int(cap_mult * mean_n)\n",
    "\n",
    "    # -------- ROS (only for classes below mean) --------\n",
    "    ros_strategy = {}\n",
    "    for cls, n in original_counts.items():\n",
    "        if n < mean_n:\n",
    "            target = min(int(n * ros_factor), mean_n)\n",
    "            if target > n:\n",
    "                ros_strategy[cls] = target\n",
    "    steps = []\n",
    "    if ros_strategy:\n",
    "        steps.append((\"ros\", RandomOverSampler(sampling_strategy=ros_strategy, random_state=random_state)))\n",
    "\n",
    "    # -------- SMOTE (after ROS, still capped at mean) --------\n",
    "    # We need to know post-ROS class counts before defining SMOTE\n",
    "    if steps:\n",
    "        X_tmp, y_tmp = Pipeline(steps).fit_resample(X_train, y_train)\n",
    "        after_ros = Counter(y_tmp)\n",
    "    else:\n",
    "        X_tmp, y_tmp = X_train, y_train\n",
    "        after_ros = original_counts\n",
    "\n",
    "    smote_strategy = {}\n",
    "    for cls, n in after_ros.items():\n",
    "        if n < mean_n:\n",
    "            target = min(int(n * smote_factor), mean_n)\n",
    "            if target > n:\n",
    "                smote_strategy[cls] = target\n",
    "\n",
    "    if smote_strategy:\n",
    "        # k_neighbors must be smaller than the size of the smallest class\n",
    "        min_class_n = min(smote_strategy.values()) if smote_strategy else 5\n",
    "        k = max(2, min(5, min_class_n - 1))  # ensure k is valid\n",
    "        steps.append((\"smote\", SMOTE(sampling_strategy=smote_strategy, random_state=random_state, k_neighbors=k)))\n",
    "\n",
    "    # -------- Undersampling (cap = 2 × mean) --------\n",
    "    after_smote_preview = Counter(Pipeline(steps).fit_resample(X_train, y_train)[1]) if steps else original_counts\n",
    "    rus_strategy = {}\n",
    "    for cls, n in after_smote_preview.items():\n",
    "        if n > undersample_cap:\n",
    "            rus_strategy[cls] = undersample_cap\n",
    "    if rus_strategy:\n",
    "        steps.append((\"rus\", RandomUnderSampler(sampling_strategy=rus_strategy, random_state=random_state)))\n",
    "\n",
    "    # -------- Final fit --------\n",
    "    if not steps:\n",
    "        # Nothing to do: dataset already balanced enough\n",
    "        return X_train, y_train\n",
    "\n",
    "    pipe = Pipeline(steps)\n",
    "    X_res, y_res = pipe.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Debug information\n",
    "    print(f\"mean: {mean_n} | Undersample cap: {undersample_cap}\")\n",
    "    print(\"Before :\", original_counts)\n",
    "    if \"ros\" in dict(steps):\n",
    "        X_ros, y_ros = Pipeline([s for s in steps if s[0] == \"ros\"]).fit_resample(X_train, y_train)\n",
    "        print(\"After ROS :\", Counter(y_ros))\n",
    "    if \"smote\" in dict(steps):\n",
    "        X_sm, y_sm = Pipeline([s for s in steps if s[0] in (\"ros\",\"smote\")]).fit_resample(X_train, y_train)\n",
    "        print(\"After SMOTE :\", Counter(y_sm))\n",
    "    if \"rus\" in dict(steps):\n",
    "        print(\"After RUS :\", Counter(y_res))\n",
    "    else:\n",
    "        print(\"After :\", Counter(y_res))\n",
    "\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ResNetMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim)\n",
    "        self.fc2 = nn.Linear(input_dim, int(input_dim * 2/3))\n",
    "        self.fc3 = nn.Linear(int(input_dim * 2/3), int(input_dim * 1/3))\n",
    "        self.fc4 = nn.Linear(int(input_dim * 1/3), output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.residual_transform = nn.Linear(input_dim, int(input_dim * 1/3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        x1 = self.relu(self.fc1(x))\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        # Residual Block\n",
    "        x2 = self.relu(self.fc2(x1))\n",
    "        x2 = self.fc3(x2)\n",
    "        x2 += self.residual_transform(x1)  # Add residual connection\n",
    "        x2 = self.relu(x2)\n",
    "\n",
    "        # Output layer\n",
    "        x3 = self.fc4(x2)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd22cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs, model_name, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(progress_bar):\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "            # Update tqdm progress bar\n",
    "            progress_bar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "        # Print final metrics on the same line\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"\\rEpoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\", end=\"\", flush=True)\n",
    "\n",
    "    # Save the entire model\n",
    "    torch.save(model.state_dict(), f\"{MODEL_OUTPUT_PATH}/{model_name}.pth\")\n",
    "    print(f\"\\n{model_name} saved successfully!\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, target_names, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9145d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82796e5",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailbox = pd.read_csv(CSV_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailbox.dropna(subset=['body', 'label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailbox.insert(0, 'hash', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailbox['body'] = df_mailbox['body'].apply(lambda x: process_body(x))\n",
    "df_mailbox['hash'] = df_mailbox['body'].apply(lambda x: calculate_hash(x.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da575b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "df_mailbox.drop_duplicates(subset=['body', 'label'], inplace=True)\n",
    "df_mailbox.dropna(inplace=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailbox.insert(3, 'vect', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77537143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize mails\n",
    "mask_not_vectorized = df_mailbox[\"vect\"].isna()\n",
    "new_bodies = df_mailbox.loc[mask_not_vectorized, \"body\"].tolist()\n",
    "\n",
    "if new_bodies:\n",
    "    indices_to_update = df_mailbox.index[mask_not_vectorized]\n",
    "\n",
    "    for idx, body in tqdm(zip(indices_to_update, new_bodies), total=len(new_bodies), desc=\"Encoding emails\"):\n",
    "        vector = VECTORIZER.encode(body, convert_to_numpy=True)\n",
    "        df_mailbox.at[idx, \"vect\"] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailbox = remove_similar_mails(df_mailbox, \"vect\", threshold=0.9, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a166175",
   "metadata": {},
   "source": [
    "### Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"df_mailbox: {len(df_mailbox)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e9691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart of the categories in df_mailbox\n",
    "plt.figure(figsize=(10, 6))\n",
    "folder_counts = df_mailbox['label'].value_counts()\n",
    "ordered_counts = [folder_counts.get(folder, 0) for folder in LABELS]\n",
    "plt.bar(LABELS, ordered_counts)\n",
    "plt.title('Distribution of Emails by Folder')\n",
    "plt.xlabel('Folder')\n",
    "plt.ylabel('Number of Emails')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc931c53",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ee675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mailbox_train, df_mailbox_test = train_test_split(df_mailbox, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df_mailbox[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(df_mailbox_train[\"vect\"].values)\n",
    "X_test = np.vstack(df_mailbox_test[\"vect\"].values)\n",
    "\n",
    "y_train = df_mailbox_train[\"label\"].map(y_encoder).values\n",
    "y_test = df_mailbox_test[\"label\"].map(y_encoder).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3206b",
   "metadata": {},
   "source": [
    "### Before oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_email_statistics_and_pca(X_train, y_train, [\"internal\", \"external\", \"spam\", \"newsletter\", \"classic phishing\", \"whaling\", \"clone\", \"blackmail\"], [\"mediumseagreen\", \"limegreen\", \"orange\", \"goldenrod\", \"red\", \"firebrick\", \"indianred\", \"lightcoral\"], 'Safe vs Suspicious mails in df_THALES_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b7ba0",
   "metadata": {},
   "source": [
    "### After oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = oversample_data(X_train, y_train, smote_factor=SMOTE_FACTOR, ros_factor=ROS_FACTOR, random_state=RANDOM_STATE, cap_mult=CAP_MULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90158a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERSAMPLED_CSV is not None:\n",
    "    df_oversampled = pd.read_csv(OVERSAMPLED_CSV)\n",
    "\n",
    "    df_oversampled.dropna(subset=['body', 'label'], inplace=True)\n",
    "    df_oversampled.insert(0, 'hash', None)\n",
    "\n",
    "    df_oversampled['body'] = df_oversampled['body'].apply(lambda x: process_body(x))\n",
    "    df_oversampled['hash'] = df_oversampled['body'].apply(lambda x: calculate_hash(x.encode('utf-8')))\n",
    "\n",
    "    df_oversampled.drop_duplicates(subset=['body', 'label'], inplace=True)\n",
    "    df_oversampled.dropna(inplace=True,axis=0)\n",
    "\n",
    "    df_oversampled.insert(3, 'vect', None)\n",
    "\n",
    "    df_oversampled['vect'] = df_oversampled['body'].apply(lambda x: VECTORIZER.encode(x, convert_to_numpy=True))\n",
    "\n",
    "    X_oversampled = np.vstack(df_oversampled[\"vect\"].values)\n",
    "    y_oversampled = df_oversampled[\"label\"].map(y_encoder).values\n",
    "\n",
    "    X_train = np.vstack((X_train, X_oversampled))\n",
    "    y_train = np.hstack((y_train, y_oversampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c716b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_email_statistics_and_pca(X_train, y_train, class_labels, class_colors, 'Safe vs Suspicious mails in df_THALES_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b78116",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d61b2",
   "metadata": {},
   "source": [
    "### Safe/Suspicious model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c499de43",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_safe_suspicious = X_train\n",
    "X_test_safe_suspicious = X_test\n",
    "y_train_safe_suspicious = y_train\n",
    "y_test_safe_suspicious = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_safe_suspicious = np.array([0 if label < 2 else 1 for label in y_train_safe_suspicious])\n",
    "y_test_safe_suspicious = np.array([0 if label < 2 else 1 for label in y_test_safe_suspicious])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_email_statistics_and_pca(X_train_safe_suspicious, y_train_safe_suspicious, safe_suspicious_labels, safe_suspicious_colors, 'Safe vs Suspicious mails in df_THALES_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67bd64",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fine-tuning data to PyTorch tensors\n",
    "X_train_safe_suspicious_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_train_safe_suspicious]), dtype=torch.float32)\n",
    "X_test_safe_suspicious_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_test_safe_suspicious]), dtype=torch.float32)\n",
    "y_train_safe_suspicious_tensor = torch.tensor(y_train_safe_suspicious, dtype=torch.long)\n",
    "y_test_safe_suspicious_tensor = torch.tensor(y_test_safe_suspicious, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "train_safe_suspicious_dataset = TensorDataset(X_train_safe_suspicious_tensor, y_train_safe_suspicious_tensor)\n",
    "test_safe_suspicious_dataset = TensorDataset(X_test_safe_suspicious_tensor, y_test_safe_suspicious_tensor)\n",
    "\n",
    "train_safe_suspicious_loader = DataLoader(train_safe_suspicious_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_safe_suspicious_loader = DataLoader(test_safe_suspicious_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404df2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train_safe_suspicious_tensor.shape[1]\n",
    "output_dim = len(np.unique(y_train_safe_suspicious))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new class weights for fine-tuning dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_safe_suspicious),\n",
    "    y=y_train_safe_suspicious\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae383f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model architecture\n",
    "model_safe_suspicious = ResNetMLP(input_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model_safe_suspicious.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model_name = f\"safe_suspicious_model\"\n",
    "train_model(model_safe_suspicious, train_safe_suspicious_loader, EPOCHS, model_name, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d72b8",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea78ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluate_model(model_safe_suspicious, test_safe_suspicious_loader, safe_suspicious_labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bddc0",
   "metadata": {},
   "source": [
    "### Unwanted/Dangerous model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d711d1",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unwanted_dangerous = X_train[np.isin(y_train, [2, 3, 4, 5, 6, 7])]\n",
    "y_train_unwanted_dangerous = y_train[np.isin(y_train, [2, 3, 4, 5, 6, 7])]\n",
    "X_test_unwanted_dangerous = X_test[np.isin(y_test, [2, 3, 4, 5, 6, 7])]\n",
    "y_test_unwanted_dangerous = y_test[np.isin(y_test, [2, 3, 4, 5, 6, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_unwanted_dangerous = np.array([0 if label < 4 else 1 for label in y_train_unwanted_dangerous])\n",
    "y_test_unwanted_dangerous = np.array([0 if label < 4 else 1 for label in y_test_unwanted_dangerous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_email_statistics_and_pca(X_train_unwanted_dangerous, y_train_unwanted_dangerous, [\"unwanted\", \"dangerous\"], [\"gold\", \"red\"], 'Unwanted vs Dangerous mails in df_THALES_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda53cba",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accf7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fine-tuning data to PyTorch tensors\n",
    "X_train_unwanted_dangerous_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_train_unwanted_dangerous]), dtype=torch.float32)\n",
    "X_test_unwanted_dangerous_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_test_unwanted_dangerous]), dtype=torch.float32)\n",
    "y_train_unwanted_dangerous_tensor = torch.tensor(y_train_unwanted_dangerous, dtype=torch.long)\n",
    "y_test_unwanted_dangerous_tensor = torch.tensor(y_test_unwanted_dangerous, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "train_unwanted_dangerous_dataset = TensorDataset(X_train_unwanted_dangerous_tensor, y_train_unwanted_dangerous_tensor)\n",
    "test_unwanted_dangerous_dataset = TensorDataset(X_test_unwanted_dangerous_tensor, y_test_unwanted_dangerous_tensor)\n",
    "\n",
    "train_unwanted_dangerous_loader = DataLoader(train_unwanted_dangerous_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_unwanted_dangerous_loader = DataLoader(test_unwanted_dangerous_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73fac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train_unwanted_dangerous_tensor.shape[1]\n",
    "output_dim = len(np.unique(y_train_unwanted_dangerous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new class weights for fine-tuning dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_unwanted_dangerous),\n",
    "    y=y_train_unwanted_dangerous\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model architecture\n",
    "model_unwanted_dangerous = ResNetMLP(input_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model_unwanted_dangerous.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5dbbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model_name = f\"unwanted_dangerous_model\"\n",
    "train_model(model_unwanted_dangerous, train_unwanted_dangerous_loader, EPOCHS, model_name, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c2068",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluate_model(model_unwanted_dangerous, test_unwanted_dangerous_loader, unwanted_dangerous_labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3795dce",
   "metadata": {},
   "source": [
    "### Safe/Suspicious and Unwanted/Dangerous models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09239357",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_global = X_train\n",
    "X_test_global = X_test\n",
    "y_train_global = y_train\n",
    "y_test_global = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38813a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_global = np.array([0 if label < 2 else 1 if label < 4 else 2 for label in y_train_global])\n",
    "y_test_global = np.array([0 if label < 2 else 1 if label < 4 else 2 for label in y_test_global])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b981368",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_email_statistics_and_pca(X_train_global, y_train_global, safe_unwanted_dangerous_labels, safe_unwanted_dangerous_colors, 'Safe vs Unwanted vs Dangerous mails in df_THALES_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5ef5e",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_global_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_test_global]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ff56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure thes model are in evaluation mode\n",
    "model_safe_suspicious.eval()\n",
    "model_unwanted_dangerous.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655847a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_SUSPICIOUS = 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_safe_suspicious_pred_logits = model_safe_suspicious(X_test_global_tensor)  # Get logits (raw outputs)\n",
    "\n",
    "    # Convert logits to probabilities for \"suspicious\" class (assuming index 1)\n",
    "    y_safe_suspicious_probs_suspicious = torch.softmax(y_safe_suspicious_pred_logits, dim=1)[:, 1]\n",
    "\n",
    "    # Classify as suspicious if the probability exceeds the threshold\n",
    "    y_safe_suspicious_pred = (y_safe_suspicious_probs_suspicious > THRESHOLD_SUSPICIOUS).int() # 1 for suspicious, 0 for safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ead62",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_DANGEROUS = 0.5\n",
    "\n",
    "# Initialize final predictions list\n",
    "final_predictions = []\n",
    "\n",
    "# Classify emails based on the first model's output\n",
    "with torch.no_grad():\n",
    "    for i, is_suspicious in enumerate(y_safe_suspicious_pred):\n",
    "        if is_suspicious.item() == 0:  # Email classified as \"safe\"\n",
    "            final_predictions.append(0)\n",
    "        else:  # Email classified as \"suspicious\", proceed to the second model\n",
    "            email_tensor = X_test_global_tensor[i].unsqueeze(0)  # Add batch dimension for a single email\n",
    "            spam_dangerous_logits = model_unwanted_dangerous(email_tensor)\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            spam_dangerous_probs = torch.softmax(spam_dangerous_logits, dim=1)\n",
    "\n",
    "            # Get the probability of \"dangerous\"\n",
    "            dangerous_prob = spam_dangerous_probs[0, 1].item()\n",
    "\n",
    "            # Classify based on the \"dangerous\" probability\n",
    "            if dangerous_prob > THRESHOLD_DANGEROUS:\n",
    "                final_predictions.append(2)  # Dangerous\n",
    "            else:\n",
    "                final_predictions.append(1)  # Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186777d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test_global, final_predictions, target_names=safe_unwanted_dangerous_labels))\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_global, final_predictions)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=safe_unwanted_dangerous_labels, yticklabels=safe_unwanted_dangerous_labels)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77abf01",
   "metadata": {},
   "source": [
    "## Precise models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dae3ce",
   "metadata": {},
   "source": [
    "### Safe model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b9cf3",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_safe = X_train[np.isin(y_train, [0, 1])]\n",
    "y_train_safe = y_train[np.isin(y_train, [0, 1])]\n",
    "X_test_safe = X_test[np.isin(y_test, [0, 1])]\n",
    "y_test_safe = y_test[np.isin(y_test, [0, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d469f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_email_statistics_and_pca(X_train_safe, y_train_safe, safe_labels, safe_colors, 'Internal vs External mails in df_THALES_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674994f",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fine-tuning data to PyTorch tensors\n",
    "X_train_safe_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_train_safe]), dtype=torch.float32)\n",
    "X_test_safe_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_test_safe]), dtype=torch.float32)\n",
    "y_train_safe_tensor = torch.tensor(y_train_safe, dtype=torch.long)\n",
    "y_test_safe_tensor = torch.tensor(y_test_safe, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "train_safe_dataset = TensorDataset(X_train_safe_tensor, y_train_safe_tensor)\n",
    "test_safe_dataset = TensorDataset(X_test_safe_tensor, y_test_safe_tensor)\n",
    "\n",
    "train_safe_loader = DataLoader(train_safe_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_safe_loader = DataLoader(test_safe_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eedab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train_safe_tensor.shape[1]\n",
    "output_dim = len(np.unique(y_train_safe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02462b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new class weights for fine-tuning dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_safe),\n",
    "    y=y_train_safe\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66840fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model architecture\n",
    "model_safe = ResNetMLP(input_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model_safe.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb57d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model_name = f\"safe_model\"\n",
    "train_model(model_safe, train_safe_loader, EPOCHS, model_name, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a3bf3",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluate_model(model_safe, test_safe_loader, safe_labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456fcbe",
   "metadata": {},
   "source": [
    "### Unwanted model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a60c9f",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unwanted = X_train[np.isin(y_train, [2, 3])]\n",
    "y_train_unwanted = y_train[np.isin(y_train, [2, 3])]\n",
    "X_test_unwanted = X_test[np.isin(y_test, [2, 3])]\n",
    "y_test_unwanted = y_test[np.isin(y_test, [2, 3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d79369",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_unwanted = np.array([0 if label == 2 else 1 for label in y_train_unwanted])\n",
    "y_test_unwanted = np.array([0 if label == 2 else 1 for label in y_test_unwanted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_email_statistics_and_pca(X_train_unwanted, y_train_unwanted, unwanted_labels, unwanted_colors, 'Spam vs Newsletter mails in df_THALES_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2cb63",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69960918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fine-tuning data to PyTorch tensors\n",
    "X_train_unwanted_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_train_unwanted]), dtype=torch.float32)\n",
    "X_test_unwanted_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_test_unwanted]), dtype=torch.float32)\n",
    "y_train_unwanted_tensor = torch.tensor(y_train_unwanted, dtype=torch.long)\n",
    "y_test_unwanted_tensor = torch.tensor(y_test_unwanted, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96253144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "train_unwanted_dataset = TensorDataset(X_train_unwanted_tensor, y_train_unwanted_tensor)\n",
    "test_unwanted_dataset = TensorDataset(X_test_unwanted_tensor, y_test_unwanted_tensor)\n",
    "\n",
    "train_unwanted_loader = DataLoader(train_unwanted_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_unwanted_loader = DataLoader(test_unwanted_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0047cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train_unwanted_tensor.shape[1]\n",
    "output_dim = len(np.unique(y_train_unwanted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4eae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new class weights for fine-tuning dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_unwanted),\n",
    "    y=y_train_unwanted\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5338a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model architecture\n",
    "model_unwanted = ResNetMLP(input_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model_unwanted.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79238dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model_name = f\"unwanted_model\"\n",
    "train_model(model_unwanted, train_unwanted_loader, EPOCHS, model_name, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23292ab7",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c55726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluate_model(model_unwanted, test_unwanted_loader, unwanted_labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9087ce9",
   "metadata": {},
   "source": [
    "### Dangerous model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05445e59",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42be25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dangerous = X_train[np.isin(y_train, [4, 5, 6, 7])]\n",
    "y_train_dangerous = y_train[np.isin(y_train, [4, 5, 6, 7])]\n",
    "X_test_dangerous = X_test[np.isin(y_test, [4, 5, 6, 7])]\n",
    "y_test_dangerous = y_test[np.isin(y_test, [4, 5, 6, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dangerous = np.array([label - 4 for label in y_train_dangerous])\n",
    "y_test_dangerous = np.array([label - 4 for label in y_test_dangerous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b37951",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_email_statistics_and_pca(X_train_dangerous, y_train_dangerous, dangerous_labels, dangerous_colors, 'Classic phishing vs Whaling vs Clone vs Blackmail mails in df_THALES_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fca070",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fine-tuning data to PyTorch tensors\n",
    "X_train_dangerous_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_train_dangerous]), dtype=torch.float32)\n",
    "X_test_dangerous_tensor = torch.tensor(np.array([np.array(row).flatten() for row in X_test_dangerous]), dtype=torch.float32)\n",
    "y_train_dangerous_tensor = torch.tensor(y_train_dangerous, dtype=torch.long)\n",
    "y_test_dangerous_tensor = torch.tensor(y_test_dangerous, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "train_dangerous_dataset = TensorDataset(X_train_dangerous_tensor, y_train_dangerous_tensor)\n",
    "test_dangerous_dataset = TensorDataset(X_test_dangerous_tensor, y_test_dangerous_tensor)\n",
    "\n",
    "train_dangerous_loader = DataLoader(train_dangerous_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dangerous_loader = DataLoader(test_dangerous_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train_dangerous_tensor.shape[1]\n",
    "output_dim = len(np.unique(y_train_dangerous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new class weights for fine-tuning dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_dangerous),\n",
    "    y=y_train_dangerous\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model architecture\n",
    "model_dangerous = ResNetMLP(input_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model_dangerous.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model_name = f\"dangerous_model\"\n",
    "train_model(model_dangerous, train_dangerous_loader, EPOCHS, model_name, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bcbcce",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluate_model(model_dangerous, test_dangerous_loader, dangerous_labels, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
